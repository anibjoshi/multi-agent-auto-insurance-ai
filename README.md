# ReAct Multi-Agent Auto Insurance Claim Processing System

A production-grade ReAct multi-agent AI system for automated auto insurance claim processing using LangGraph, tool calling, and OpenAI GPT models.

## ğŸ¯ Overview

This system implements a sophisticated **ReAct (Reasoning + Acting) multi-agent workflow** that processes auto insurance claims through 10 specialized AI agents. Each agent uses the ReAct pattern with tool calling to reason about claims and access structured data, providing transparent decision-making with detailed reasoning trails. The system uses LangGraph for orchestration and provides both programmatic and REST API interfaces.

## âœ¨ Features

- **ğŸ¤– 10 Specialized ReAct Agents**: Each agent uses reasoning + acting pattern with domain-specific tools
- **ğŸ”§ Tool-Based Data Access**: Structured tools replace raw JSON prompts for reliable data access
- **ğŸ§  Advanced Reasoning**: Step-by-step reasoning trails with tool usage documentation
- **âš¡ Parallel Processing**: ReAct agents run concurrently for fast claim processing
- **ğŸ”„ LangGraph Orchestration**: Production-grade workflow management with state isolation
- **ğŸŒ REST API**: FastAPI-based web service with interactive documentation
- **ğŸ“Š Detailed Audit Trails**: Complete reasoning and tool usage history for each decision
- **ğŸš€ Batch Processing**: Handle multiple claims efficiently with proper data isolation
- **ğŸ“ˆ Production Ready**: Comprehensive error handling, logging, and monitoring

## ğŸ—ï¸ ReAct System Architecture

The system uses a **ReAct (Reasoning + Acting) pattern** with tool calling for sophisticated claim analysis:

### ReAct Pattern Benefits
- **ğŸ§  Reasoning**: Agents think step-by-step about claim requirements
- **ğŸ”§ Acting**: Agents use specialized tools to gather and analyze data
- **ğŸ“Š Transparency**: Complete reasoning and tool usage audit trail
- **ğŸ¯ Accuracy**: Structured data access reduces hallucination risks

### Stage 1: Parallel ReAct Agent Processing
Nine specialized ReAct agents analyze the claim simultaneously using domain-specific tools:

1. **PolicyValidator** - Validates policy timing and eligibility using policy and date calculation tools
2. **DocumentValidator** - Ensures required documents using claim info and documentation tools
3. **DriverVerifier** - Verifies driver eligibility using driver information and coverage tools
4. **VehicleDamageEvaluator** - Assesses damage using vehicle data and total loss calculation tools
5. **CoverageEvaluator** - Enforces limits using policy, vehicle, coverage, and liability tools
6. **CatastropheChecker** - Handles CAT events using claim and catastrophe information tools
7. **LiabilityAssessor** - Allocates fault using liability and documentation tools
8. **RentalBenefitChecker** - Evaluates rental benefits using rental and coverage tools
9. **FraudDetector** - Detects fraud using vehicle, mileage, CAT, and timing analysis tools

### Stage 2: Final ReAct Decision
10. **ClaimDecider** - Uses ReAct pattern to aggregate all agent responses and make final decisions

### Tool Categories
- **ğŸ“‹ Data Access Tools**: `get_claim_basic_info`, `get_policy_information`, `get_driver_information`, etc.
- **ğŸ” Analysis Tools**: `check_total_loss_threshold`, `check_mileage_discrepancy`
- **ğŸ“… Calculation Tools**: `calculate_days_between_dates`
- **ğŸ·ï¸ Specialized Tools**: `get_catastrophe_information`, `get_liability_information`, etc.

## ğŸš€ Quick Start

1. **Install the Package**
   ```bash
   # Install with pip
   pip install -e .
   
   # Or use make (recommended for development)
   make install
   ```

2. **Configure Environment**
   Create a `.env` file:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   OPENAI_MODEL=gpt-4-turbo-preview
   ```

3. **Test the System**
   ```bash
   make test
   # Or directly: python tests/test_system.py
   ```

4. **Run ReAct Demo**
   ```bash
   make demo
   # Or directly: python scripts/demo_react.py
   
   # Run the new ReAct-specific demo
   python scripts/demo_react.py
   ```

5. **Process Datasets** (for benchmarking)
   ```bash
   # Convert datasets to scalable JSONL format
   make process-datasets
   ```

6. **Run Accuracy Benchmarks**
   ```bash
   # Quick test (5 claims for development)
   make quick-benchmark
   
   # Full benchmark (customizable)
   make benchmark
   
   # Full benchmark on 450 claims
   make benchmark-full
   ```

7. **Manage Prompts (Optional)**
   ```bash
   # List all available prompts  
   PYTHONPATH=. python scripts/prompt_manager.py list
   
   # View a specific agent's prompt
   PYTHONPATH=. python scripts/prompt_manager.py show --agent PolicyValidator
   
   # Validate all prompts
   PYTHONPATH=. python scripts/prompt_manager.py validate
   ```

8. **Start API Server**
   ```bash
   make api
   # Or directly: python api/main.py
   ```

9. **Access Documentation**
   Visit `http://localhost:8000/docs` for interactive API documentation

## ğŸ“– Usage Example

### Process a Claim via API

```python
import requests

claim_data = {
    "claim_id": "CLM-2025-001",
    "incident_date": "2025-01-20",
    "report_date": "2025-01-22",
    "state": "TX",
    "driver_name": "John Doe",
    "damage_type": "collision",
    "repair_estimate": 5000,
    # ... other required fields
}

response = requests.post(
    "http://localhost:8000/claims/process",
    json={"claim_data": claim_data, "include_agent_details": True}
)

result = response.json()
print(f"Decision: {result['final_decision']['status']}")
print(f"Reason: {result['final_decision']['explanation']}")
```

### Process Claims Programmatically

```python
from src import ClaimProcessingWorkflow, ClaimData

# Initialize workflow
workflow = ClaimProcessingWorkflow(openai_api_key="your_key_here")

# Process claim
claim = ClaimData(**claim_data)
result = await workflow.process_claim(claim)

print(f"Final Decision: {result.final_decision.status}")
for response in result.agent_responses:
    print(f"{response.agent}: {response.status} - {response.explanation}")
```

## ğŸ”§ Configuration

The system supports various configuration options through environment variables:

- **OpenAI Settings**: API key, model selection, temperature
- **API Configuration**: Host, port, CORS settings
- **Logging**: Log levels, output formats
- **Performance**: Batch sizes, timeout settings

See [SETUP.md](SETUP.md) for detailed configuration instructions.

## ğŸ“Š Agent Decision Logic

Each agent implements specific business rules:

- **APPROVED**: All criteria met, proceed with claim
- **REJECTED**: Critical failure, deny claim
- **PARTIAL**: Approve with limitations or reduced amount
- **ESCALATE**: Requires human review or additional investigation

The ClaimDecider aggregates responses using priority rules:
1. Any REJECTED â†’ Overall REJECTED
2. Any ESCALATE â†’ Overall ESCALATE  
3. Any PARTIAL â†’ Overall PARTIAL
4. All APPROVED â†’ Overall APPROVED

## ğŸ“ Prompt Management System

The system uses **file-based prompts** co-located with agent code for easy editing and version control.

### Key Benefits
- **ğŸ”„ Easy Iteration**: Modify prompts without touching code
- **ğŸ‘¥ Team Collaboration**: Non-developers can improve prompts
- **ğŸ§¹ Clean Architecture**: Prompts live alongside agent implementations

### Prompt Structure
```
src/agents/                      # ReAct prompts + agent code (co-located)
â”œâ”€â”€ policy_validator.py         # Python agent implementation
â”œâ”€â”€ policy_validator.md         # ReAct prompt file
â”œâ”€â”€ fraud_detector.py           # Python agent implementation  
â”œâ”€â”€ fraud_detector.md           # ReAct prompt file
â”œâ”€â”€ claim_decider.py            # Python agent implementation
â”œâ”€â”€ claim_decider.md            # ReAct prompt file
â””â”€â”€ ...                         # (co-located .py and .md files)

agents/                         # Legacy reference
â”œâ”€â”€ v1/                        # Original prompts
â””â”€â”€ v2/                        # Traditional prompts
```

### Simple Management
```bash
# List all prompts
PYTHONPATH=. python scripts/prompt_manager.py list

# View specific prompt
PYTHONPATH=. python scripts/prompt_manager.py show --agent PolicyValidator

# Validate prompts
PYTHONPATH=. python scripts/prompt_manager.py validate
```

### ReAct Prompt Format
Each prompt includes:
- **Role Definition**: Clear agent responsibilities
- **Available Tools**: Domain-specific tool descriptions  
- **Business Rules**: Systematic decision criteria
- **ReAct Process**: Step-by-step reasoning framework
- **Output Format**: Consistent JSON response structure

## ğŸ›¡ï¸ Production Considerations

- **Security**: API key management, HTTPS, authentication
- **Scaling**: Rate limiting, request queuing, load balancing
- **Monitoring**: Logging, metrics, health checks
- **Compliance**: Data privacy, audit trails, regulatory requirements

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ models.py                # Data models and schemas
â”‚   â”œâ”€â”€ workflow.py              # LangGraph workflow orchestration
â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”œâ”€â”€ tools.py                 # ReAct agent tools
â”‚   â”œâ”€â”€ prompt_loader.py         # Prompt file management
â”‚   â”œâ”€â”€ py.typed                 # Type checking marker
â”‚   â””â”€â”€ agents/                  # Agent implementations and prompts
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py              # Base agent classes
â”‚       â”œâ”€â”€ policy_validator.py  # Agent implementation
â”‚       â”œâ”€â”€ policy_validator.md  # ReAct prompt file
â”‚       â”œâ”€â”€ fraud_detector.py    # Agent implementation
â”‚       â”œâ”€â”€ fraud_detector.md    # ReAct prompt file
â”‚       â””â”€â”€ ...                  # (all agents with co-located .py and .md files)
â”œâ”€â”€ api/                          # REST API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                  # FastAPI application
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_system.py           # Comprehensive tests
â”‚   â””â”€â”€ test_react_agents.py     # ReAct agent unit tests
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ demo_react.py            # ReAct agent demonstration
â”‚   â””â”€â”€ prompt_manager.py        # Prompt management utility
â”œâ”€â”€ docs/                         # Documentation
â”‚   â””â”€â”€ SETUP.md                 # Setup instructions
â”œâ”€â”€ agents/                       # Legacy prompt definitions
â”‚   â”œâ”€â”€ v1/                      # Version 1 prompts
â”‚   â””â”€â”€ v2/                      # Version 2 prompts
â”œâ”€â”€ dataset/                      # Sample data
â”‚   â”œâ”€â”€ auto_claim_sample_inputs.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ setup.py                      # Package setup (legacy)
â”œâ”€â”€ pyproject.toml               # Modern package configuration
â”œâ”€â”€ Makefile                     # Development commands
â””â”€â”€ README.md                    # This file
```

## ğŸ¤ Contributing

This is a baseline implementation demonstrating multi-agent claim processing. Areas for enhancement:

- **Advanced Reasoning**: Integration with specialized insurance databases
- **Machine Learning**: Pattern recognition for fraud detection
- **Integration**: Connect with existing insurance systems
- **UI/UX**: Web interface for claim adjusters
- **Analytics**: Business intelligence and reporting features

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸš€ Getting Started

Ready to process your first claim? Check out [SETUP.md](SETUP.md) for detailed instructions or run the test script to see the system in action!
